{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenazation example with 'Kupfer, blank I, Millberry'\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "import de_dep_news_trf\n",
    "import en_core_web_sm\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "de_nlp = spacy.load('de_dep_news_trf')\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object for testing\n",
    "doc = de_nlp('Kupfer, blank I, Millberry')\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Print results\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The next step is to tokenize our data. Tokenization is the process of separating our text into tokens, which can be defined as one or multiple units. There are several techniques to do this. These are the steps we are going to take in preprocessing:\n",
    "\n",
    "1. Converting words into lowercase\n",
    "2. Removing leading and trailing whitespaces\n",
    "3. Converting words into lemmas\n",
    "4. Removing punctuation\n",
    "5. Removing stopwords\n",
    "6. Expanding contractions\n",
    "7. Removing special characters\n",
    "\n",
    "Note that most of our text is in German. Therefore, we need to leverage the parameters of the [spaCy documentation](https://spacy.io/usage/models)\n",
    "Because of this, we will need an algorithm to determine in what language the text is (or is likely to be) to perform the best possible tokenization.\n",
    "\n",
    "_Note, after loading the spacy package you might need to reboot your computer (the first time). If you are getting this error:_\n",
    "> ValueError: [E002] Can't find factory for 'transformer' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the baseline exercise, we will use standard stopwords. However, a potential improvement is to build custom stopword lists tailored to the use-case of scrap metal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get german default stopwords\n",
    "de_stopwords  = spacy.lang.de.stop_words.STOP_WORDS\n",
    "\n",
    "# Get english default stopwords\n",
    "en_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we will apply a preprocessing step that converts words to lemmas. Lemmas are a basic form of a word that removes inflexions and therefore decreases variance. For example, the lemma for a verb is its infinitive form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uninteresting_char = set([',', '(', ')', 'und'])\n",
    "\n",
    "def preprocess(text, language='en'):\n",
    "    # Create Doc object\n",
    "        if language == 'de':\n",
    "                doc = de_nlp(text, disable=['ner', 'parser'])\n",
    "                stopwords = de_stopwords\n",
    "        else:\n",
    "                doc = en_nlp(text, disable=['ner', 'parser'])\n",
    "                stopwords = en_stopwords\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        # Remove stopwords and non-alphabetic characters\n",
    "        a_lemmas = [lemma for lemma in lemmas \n",
    "                if lemma not in stopwords\n",
    "                and lemma not in uninteresting_char]\n",
    "        return ' '.join(a_lemmas)\n",
    "\n",
    "# Apply preprocess to the mateirial alias given the language\n",
    "df1['p_ds_material_alias'] = df1.apply(lambda x: preprocess(x['ds_material_alias'], x['cd_alias_language']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = df1['p_ds_material_alias'].values\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 50 words frequencies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sum the frequencies of each word\n",
    "word_freq = bow_df.sum()\n",
    "\n",
    "# Plot the top 50 words\n",
    "word_freq.sort_values(ascending=False).head(50).plot(kind='bar', figsize=(15, 7))\n",
    "plt.title('Top 50 words frequencies')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Words')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "## Calculating the cosine similarity\n",
    "\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix,tfidf_matrix1)\n",
    "\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We should create word embeddings for the different forms of metals. For instance:\n",
    "Kupfer, Copper, Cu should all be the same concept. --> Would increase the performance of the model without the need of so much data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = material_desc_df.drop('product_id', axis=1)\n",
    "y = material_desc_df['product_id']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Corpus\n",
    "corpus_orig = material_desc_df['text'].to_list()\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix,tfidf_matrix)\n",
    "\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(material_desc_df.index, index=material_desc_df['product_id']).drop_duplicates()\n",
    "\n",
    "# Create function to return top n matches\n",
    "def get_top_n_matches(material, cosine_sim, indices, n=10):\n",
    "    # Add new material to corpus\n",
    "    corpus = corpus_orig.append(material)\n",
    "    # Construct the TF-IDF matrix\n",
    "    tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "    # Generate the cosine similarity matrix\n",
    "    cosine_sim = linear_kernel(tfidf_matrix,tfidf_matrix)\n",
    "    # Order the results by similarity\n",
    "    sim_scores = list(enumerate(cosine_sim[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cart_items\n",
    "materials = pd.read_csv('data_git/material_aliases.csv')\n",
    "\n",
    "materials['cd_product_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Corpus\n",
    "corpus = material_desc_df['text'].to_list()\n",
    "\n",
    "# Material to test\n",
    "material = 'Copper milb'\n",
    "\n",
    "# Add new material to corpus\n",
    "corpus.append(material)\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix,tfidf_matrix)\n",
    "\n",
    "# Get scores of the last material\n",
    "sim_scores = list(enumerate(cosine_sim[-1]))\n",
    "\n",
    "# Sort the materials based on the similarity scores\n",
    "sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the scores for 10 most similar materials\n",
    "sim_scores = sim_scores[1:11]\n",
    "\n",
    "# Get the material indices\n",
    "material_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "# Return the top 10 most similar materials\n",
    "top_n_scores = material_desc_df['product_id'].iloc[material_indices]\n",
    "\n",
    "# Create DataFrame with top 10 most similar materials and their scores\n",
    "top_n_scores_df = pd.DataFrame({'product_id': top_n_scores, 'similarity_score': [i[1] for i in sim_scores]})\n",
    "\n",
    "# Merge with materials DataFrame to get material details\n",
    "result = pd.merge(top_n_scores_df, materials, on='product_id')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), \"data/\")\n",
    "\n",
    "# Load the data\n",
    "materials = pd.read_csv(path + 'materials.csv')\n",
    "material_desc_df = pd.read_csv(path + 'material_desc.csv')\n",
    "\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Corpus\n",
    "corpus = material_desc_df['text'].to_list()\n",
    "\n",
    "# Material to test\n",
    "material = 'Copper milb'\n",
    "\n",
    "# Add new material to corpus\n",
    "corpus.append(material)\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix,tfidf_matrix)\n",
    "\n",
    "# Get scores of the last material\n",
    "sim_scores = list(enumerate(cosine_sim[-1]))\n",
    "\n",
    "# Sort the materials based on the similarity scores\n",
    "sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the scores for 10 most similar materials\n",
    "sim_scores = sim_scores[1:11]\n",
    "\n",
    "# Get the material indices\n",
    "material_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "# Return the top 10 most similar materials\n",
    "top_n_scores = material_desc_df['product_id'].iloc[material_indices]\n",
    "\n",
    "# Create DataFrame with top 10 most similar materials and their scores\n",
    "top_n_scores_df = pd.DataFrame({'product_id': top_n_scores, 'similarity_score': [i[1] for i in sim_scores]})\n",
    "\n",
    "# Merge with materials DataFrame to get material details\n",
    "result = pd.merge(top_n_scores_df, materials, on='product_id')\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
